## 残差网络 ResNets

#### 1.残差简介

“残差在数理统计中是指实际观察值与估计值（拟合值）之间的差。”   “如果回归模型正确的话， 我们可以将残差看作误差的观测值。”

　　更准确地，假设我们想要找一个 x，使得 f(x)=b，给定一个 x 的估计值 x0，残差（residual）就是 b−f(x0)，同时，误差就是 x−x0。

　　即使 x 不知道，我们仍然可以计算残差，只是不能计算误差罢了。







如图 2 所示，xx 表示输入，F(x)F(x) 表示残差块在第二层激活函数之前的输出，即 F(x)=W2σ(W1x)F(x)=W2σ(W1x)，其中 W1W1 和 W2W2 表示第一层和第二层的权重，σσ 表示 ReLU 激活函数。（这里省略了 bias。）最后残差块的输出是 σ(F(x)+x)σ(F(x)+x)。

　　当没有 shortcut connection（即图 2 右侧从 xx 到 ⨁⨁ 的箭头）时，残差块就是一个普通的 2 层网络。残差块中的网络可以是全连接层，也可以是卷积层。设第二层网络在激活函数之前的输出为 H(x)H(x)。如果在该 2 层网络中，最优的输出就是输入 xx，那么对于没有 shortcut connection 的网络，就需要将其优化成 H(x)=xH(x)=x；对于有 shortcut connection 的网络，即残差块，最优输出是 xx，则只需要将 F(x)=H(x)−xF(x)=H(x)−x 优化为 0 即可。后者的优化会比前者简单。这也是残差这一叫法的由来。