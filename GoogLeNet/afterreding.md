### Going deeper with convolutions

DOI：10.1109/CVPR.2015.7298594

年份：2014

#### Abstract:

​        作者在本文中提出了一个深度卷积神经网络架构叫做Inception，它是为了在ILSVRC14比赛中进行**图像识别**和**目标检测**的任务。这种架构的特点是更好的改善了网络中的计算资源的利用，可以在**增加网络深度和宽度的情况下而保持计算开支为一个常数**。GoogLeNet是一个22层的网络，作者在分类与检测任务中检测了它的性能。

#### 1.Introduction

​        近年来由于**卷积神经网络**的发展，图像识别和目标检测发展迅速。这些进步不仅是更强大硬件、更大数据集或者更大网络模型，还有一些列新的想法，算法还有改进网络结构。GoogLeNet比AlexNet少12x的参数，但是却比他更精确。目标检测的最大进步不仅仅是更深的网络或者更大的模型，而是将**深度架构**与**计算机视觉**结合，比如像R-CNN一样。

​        另一个重要的因素是我们的算法由于移动和嵌入式计算的发展而变得更加有效。值得注意的本篇文章中的架构是经过思考的，而不仅仅是有一个精确度上的提升。文中设计的架构计算开支可以达到**1.5M次加-乘**，不仅仅是在学术上提高精确度，而且可以推广到真实世界使用。即使在一个很大的数据集上，它也能保持一个合理的开销。

​        “深”有两层含义，一个是“Inception” 模块达到了一个新的水平，另一个是比较直观的意思，网络的深度越来越大。在ILSVRC14上，GoogLeNet的表现确实比目前最先进的网络更出色。

#### 2.Related Work

​        从LeNet5开始，卷积神经网络一般有一个标准的结构--堆叠卷积神经网络（也可以后面跟着正则化或者最大池化），最后是全连接网络。基于基本设计的变体开始在论文中流行起来，并且在基本数据集中表现出色。对于更大的网络，最近的趋势是增大层数和每层中的单元数，并且使用dropout来解决过拟合的问题。

​        尽管担心最大池化会丢失空间精度信息，像AlexNet类似的网络已经用来做定位、检测和人类姿势估计。有些人利用不同规格的过滤器来处理不同大小的图片，像Inception模块类似。然而不像两层固定的模型，Inception中所有模块是学习得到的。此外Inception模块重复多次，就得到了22层的GoogLeNet。

​        Network-in-Network是为了提高神经网络表达力而提出的。当使用卷积神经网络的时候，它会被当做额外的1x1卷积网络放在ReLU激活函数之前，这样可以使它很容易的在CNN整个管道中继承，我们使用了很多这样的网络。在我们的设计中，1x1的网络有双重的目的，最重要的是他可以主要用来实现降维来消除计算瓶颈，否则就会限制我们网络的大小。这样不但可以提高深度，并且可以提高广度而没有性能上的损失。

​        目前最主流的目标检测是R-CNN，它把整个检测问题分为两部分，

#### 1.Introduction

#### 1.Introduction

#### 1.Introduction

#### 1.Introduction

#### 1.Introduction

